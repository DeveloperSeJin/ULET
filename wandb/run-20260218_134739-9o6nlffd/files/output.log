[34m[1mwandb[0m: Detected [huggingface_hub.inference] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
Epoch 1/100
----------
  0%|                                                               | 0/341 [00:02<?, ?it/s]
torch.Size([32, 32, 1024])
Traceback (most recent call last):
  File "/home/saul_park/workspace/code/ULET/train.py", line 393, in <module>
    model = train_model(dataloaders, device, model, optimizer_step1, exp_lr_scheduler_step1, num_epochs=num_epochs_step1,
  File "/home/saul_park/workspace/code/ULET/train.py", line 79, in train_model
    _, out, loss = model(batch= (input_embeddings_batch,target_ids_batch),input_masks_batch= input_masks_batch)
  File "/home/saul_park/miniconda3/envs/wordlevel/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/saul_park/workspace/code/ULET/eegpt.py", line 288, in forward
    out = self.pretrained_LM(inputs_embeds = logit, attention_mask = input_masks_batch,
  File "/home/saul_park/miniconda3/envs/wordlevel/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/saul_park/miniconda3/envs/wordlevel/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1577, in forward
    outputs = self.model(
  File "/home/saul_park/miniconda3/envs/wordlevel/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/saul_park/miniconda3/envs/wordlevel/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1445, in forward
    encoder_outputs = self.encoder(
  File "/home/saul_park/miniconda3/envs/wordlevel/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/saul_park/miniconda3/envs/wordlevel/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1074, in forward
    layer_outputs = encoder_layer(
  File "/home/saul_park/miniconda3/envs/wordlevel/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/saul_park/miniconda3/envs/wordlevel/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 537, in forward
    hidden_states, attn_weights, _ = self.self_attn(
  File "/home/saul_park/miniconda3/envs/wordlevel/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/saul_park/miniconda3/envs/wordlevel/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 241, in forward
    raise ValueError(
ValueError: Attention mask should be of size (32, 1, 32, 32), but is torch.Size([32, 1, 56, 56])
